{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eoO-e87BVTW"
      },
      "outputs": [],
      "source": [
        "# Notebook to be executed entirely in Google Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EelolO7pCDjB"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers pillow numpy scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MbtbKrWMCHqD"
      },
      "outputs": [],
      "source": [
        "# Colab Cell 3: Authenticate Kaggle API and Download Dataset\n",
        "\n",
        "# Create a directory for Kaggle configuration\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Copy the uploaded kaggle.json to the correct directory\n",
        "# Make sure 'kaggle.json' is in /content/ (uploaded via the file browser)\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set appropriate permissions for the Kaggle API key (important for security)\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# --- Download the Sketchy Dataset from Kaggle ---\n",
        "# Define a directory in Colab's temporary storage for downloads\n",
        "# This is *not* on your Google Drive initially, but faster for downloads\n",
        "DOWNLOAD_PATH = \"/content/data\" # Ensure this variable is clean\n",
        "\n",
        "# Create the download directory if it doesn't exist\n",
        "!mkdir -p \"$DOWNLOAD_PATH\"\n",
        "\n",
        "# Change current directory to where we want to download\n",
        "%cd \"$DOWNLOAD_PATH\"\n",
        "\n",
        "# Replace with the exact Kaggle API command for YOUR chosen Sketchy dataset if different\n",
        "# You can find this command on the dataset's Kaggle page under '...' -> 'Copy API command'\n",
        "KAGGLE_DATASET_SLUG = \"dhananjayapaliwal/fulldataset\" # This is a common Sketchy dataset ID\n",
        "!kaggle datasets download -d {KAGGLE_DATASET_SLUG} --unzip\n",
        "\n",
        "print(\"\\n--- Listing extracted contents to verify path ---\")\n",
        "# List contents of the download directory to confirm extraction\n",
        "!ls -F \"$DOWNLOAD_PATH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b_2pm8jyxbvO"
      },
      "outputs": [],
      "source": [
        "# Colab Cell 3: Authenticate Kaggle API and Download Dataset\n",
        "\n",
        "# Create a directory for Kaggle configuration\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Copy the uploaded kaggle.json to the correct directory\n",
        "# Make sure 'kaggle.json' is in /content/ (uploaded via the file browser)\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set appropriate permissions for the Kaggle API key (important for security)\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# --- Download the Sketchy Dataset from Kaggle ---\n",
        "# Define a directory in Colab's temporary storage for downloads\n",
        "# This is *not* on your Google Drive initially, but faster for downloads\n",
        "DOWNLOAD_PATH = \"/content/data\" # Ensure this variable is clean\n",
        "\n",
        "# Change current directory to where we want to download\n",
        "%cd \"$DOWNLOAD_PATH\"\n",
        "\n",
        "# Replace with the exact Kaggle API command for YOUR chosen Sketchy dataset if different\n",
        "# You can find this command on the dataset's Kaggle page under '...' -> 'Copy API command'\n",
        "KAGGLE_DATASET_SLUG = \"rishikashili/tuberlin\" # This is a common Sketchy dataset ID\n",
        "!kaggle datasets download -d {KAGGLE_DATASET_SLUG} --unzip\n",
        "\n",
        "print(\"\\n--- Listing extracted contents to verify path ---\")\n",
        "# List contents of the download directory to confirm extraction\n",
        "!ls -F \"$DOWNLOAD_PATH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZdQZEg4lYOLt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "\n",
        "# --- Set seed for reproducibility ---\n",
        "random.seed(42)\n",
        "\n",
        "# --- Paths ---\n",
        "KAGGLE_EXTRACTED_FOLDER_NAME = \"temp_extraction\"\n",
        "BASE_DATA_DIR = os.path.join(\"/content/data\", KAGGLE_EXTRACTED_FOLDER_NAME)\n",
        "TRAIN_SKETCH_PATH = os.path.join(BASE_DATA_DIR, \"256x256\", \"splitted_sketches\", \"train\", \"tx_000100000000\")\n",
        "TRAIN_PHOTO_PATH = os.path.join(BASE_DATA_DIR, \"256x256\", \"photo\", \"tx_000100000000\")\n",
        "GOOGLE_DRIVE_ROOT = \"/content/drive/My Drive\"\n",
        "EMBEDDINGS_DIR_ON_DRIVE = os.path.join(GOOGLE_DRIVE_ROOT, \"ML_Embeddings_SBIR_PyTorch\")\n",
        "os.makedirs(EMBEDDINGS_DIR_ON_DRIVE, exist_ok=True)\n",
        "PROJECTION_NET_SAVE_PATH = os.path.join(EMBEDDINGS_DIR_ON_DRIVE, \"sketch_projection_net_category_infonce.pth\")\n",
        "ALL_CATEGORIES = sorted([cat for cat in os.listdir(TRAIN_SKETCH_PATH) if os.path.isdir(os.path.join(TRAIN_SKETCH_PATH, cat))])\n",
        "\n",
        "print(f\"Total categories found: {len(ALL_CATEGORIES)}\")\n",
        "\n",
        "# --- Split into 100 train and 25 test categories ---\n",
        "train_categories = random.sample(ALL_CATEGORIES, 100)\n",
        "test_categories = [cat for cat in ALL_CATEGORIES if cat not in train_categories]\n",
        "\n",
        "print(f\"Train categories: {len(train_categories)}\")\n",
        "print(f\"Test categories: {len(test_categories)}\")\n",
        "\n",
        "# --- Save to JSON for later use ---\n",
        "SPLIT_SAVE_PATH = os.path.join(EMBEDDINGS_DIR_ON_DRIVE, \"category_split.json\")\n",
        "with open(SPLIT_SAVE_PATH, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"train_categories\": train_categories,\n",
        "        \"test_categories\": test_categories\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"Category split saved at {SPLIT_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pWb19cY3CPVD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import json\n",
        "\n",
        "# --- Device Configuration ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Load Pre-trained CLIP Model (Frozen) ---\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "CLIP_EMBEDDING_DIM = 512\n",
        "\n",
        "# --- Dataset Class for Category-level Training ---\n",
        "class CategorySketchPhotoDataset(Dataset):\n",
        "    def __init__(self, sketch_dir, photo_dir, category_list):\n",
        "        self.sketch_files = []\n",
        "        self.sketch_labels = []\n",
        "        self.photo_files = []\n",
        "        self.photo_labels = []\n",
        "        self.category_to_idx = {cat: idx for idx, cat in enumerate(category_list)}\n",
        "\n",
        "        # Load sketches\n",
        "        for category in category_list:\n",
        "            cat_path = os.path.join(sketch_dir, category)\n",
        "            if not os.path.isdir(cat_path):\n",
        "                continue\n",
        "            files = glob.glob(os.path.join(cat_path, '*'))\n",
        "            self.sketch_files.extend(files)\n",
        "            self.sketch_labels.extend([self.category_to_idx[category]] * len(files))\n",
        "\n",
        "        # Load photos\n",
        "        for category in category_list:\n",
        "            cat_path = os.path.join(photo_dir, category)\n",
        "            if not os.path.isdir(cat_path):\n",
        "                continue\n",
        "            files = glob.glob(os.path.join(cat_path, '*'))\n",
        "            self.photo_files.extend(files)\n",
        "            self.photo_labels.extend([self.category_to_idx[category]] * len(files))\n",
        "\n",
        "        # Build category index for faster sampling\n",
        "        self.photo_by_category = {}\n",
        "        for file, label in zip(self.photo_files, self.photo_labels):\n",
        "            if label not in self.photo_by_category:\n",
        "                self.photo_by_category[label] = []\n",
        "            self.photo_by_category[label].append(file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sketch_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sketch_path = self.sketch_files[idx]\n",
        "        label = self.sketch_labels[idx]\n",
        "        sketch_img = Image.open(sketch_path).convert(\"RGB\")\n",
        "\n",
        "        # Sample a random photo from the same category\n",
        "        photo_path = np.random.choice(self.photo_by_category[label])\n",
        "        photo_img = Image.open(photo_path).convert(\"RGB\")\n",
        "\n",
        "        return sketch_img, photo_img, label\n",
        "\n",
        "# --- Faster Collate Function ---\n",
        "def fast_collate_fn(batch):\n",
        "    sketches, photos, labels = zip(*batch)\n",
        "    sketch_inputs = processor(images=list(sketches), return_tensors=\"pt\", padding=True)\n",
        "    photo_inputs = processor(images=list(photos), return_tensors=\"pt\", padding=True)\n",
        "    return sketch_inputs['pixel_values'], photo_inputs['pixel_values'], torch.tensor(labels)\n",
        "\n",
        "# --- Setup Dataset and Dataloader ---\n",
        "# Load category split\n",
        "SPLIT_SAVE_PATH = os.path.join(EMBEDDINGS_DIR_ON_DRIVE, \"category_split.json\")\n",
        "with open(SPLIT_SAVE_PATH, \"r\") as f:\n",
        "    split = json.load(f)\n",
        "\n",
        "train_categories = split[\"train_categories\"]\n",
        "\n",
        "# Initialize dataset with train categories only\n",
        "train_dataset = CategorySketchPhotoDataset(TRAIN_SKETCH_PATH, TRAIN_PHOTO_PATH, split[\"train_categories\"])\n",
        "print(f\"Total sketch-photo category pairs loaded: {len(train_dataset)}\")\n",
        "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, collate_fn=fast_collate_fn)\n",
        "\n",
        "# --- Define the Sketch Projection Network ---\n",
        "class SketchProjectionNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SketchProjectionNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "projection_net = SketchProjectionNet(CLIP_EMBEDDING_DIM, CLIP_EMBEDDING_DIM).to(device)\n",
        "optimizer = optim.Adam(projection_net.parameters(), lr=1e-4)\n",
        "\n",
        "# --- InfoNCE Loss ---\n",
        "def info_nce_loss(sketch_embeds, photo_embeds, temperature=0.07):\n",
        "    sketch_embeds = F.normalize(sketch_embeds, p=2, dim=-1)\n",
        "    photo_embeds = F.normalize(photo_embeds, p=2, dim=-1)\n",
        "\n",
        "    logits = torch.matmul(sketch_embeds, photo_embeds.T) / temperature\n",
        "    labels = torch.arange(logits.size(0)).to(device)\n",
        "    loss = F.cross_entropy(logits, labels)\n",
        "    return loss\n",
        "\n",
        "# --- Training Loop ---\n",
        "NUM_EPOCHS = 5\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    projection_net.train()\n",
        "    total_loss = 0\n",
        "    for sketches, photos, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
        "        sketches, photos = sketches.to(device), photos.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sketch_embeds = model.get_image_features(sketches)\n",
        "            photo_embeds = model.get_image_features(photos)\n",
        "\n",
        "        projected = projection_net(sketch_embeds)\n",
        "\n",
        "        loss = info_nce_loss(projected, photo_embeds)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed. Avg Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# --- Save Projection Network ---\n",
        "torch.save(projection_net.state_dict(), PROJECTION_NET_SAVE_PATH)\n",
        "print(f\"Model saved at {PROJECTION_NET_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fm15m4jwN-cW"
      },
      "outputs": [],
      "source": [
        "# Colab Cell 5: Evaluate with the Trained Projection Network\n",
        "\n",
        "import torch\n",
        "# --- CORRECTED IMPORTS HERE ---\n",
        "import torch.nn as nn # <-- Added\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image # <-- Added\n",
        "import os\n",
        "from tqdm import tqdm # <-- Added (though not used in eval, good practice if modified)\n",
        "import random # <-- Added\n",
        "import glob # <-- Added (for finding random sketch)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Device Configuration ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Load Pre-trained CLIP Model (Frozen for inference) ---\n",
        "print(\"\\nLoading CLIP model for inference...\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model.to(device)\n",
        "for param in model.parameters(): # Ensure it's frozen again if running independently\n",
        "    param.requires_grad = False\n",
        "print(\"CLIP model loaded.\")\n",
        "\n",
        "# --- Define the Sketch Projection Network (Must be defined again in this cell) ---\n",
        "class SketchProjectionNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SketchProjectionNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "CLIP_EMBEDDING_DIM = 512\n",
        "projection_net = SketchProjectionNet(CLIP_EMBEDDING_DIM, CLIP_EMBEDDING_DIM).to(device)\n",
        "\n",
        "# --- Define Dataset and Embedding Paths (GLOBAL TO THIS CELL) ---\n",
        "KAGGLE_EXTRACTED_FOLDER_NAME = \"temp_extraction\" # Must be consistent with Cell 3 & 4\n",
        "BASE_DATA_DIR = os.path.join(\"/content/data\", KAGGLE_EXTRACTED_FOLDER_NAME)\n",
        "\n",
        "# Paths for saving/loading embeddings on Google Drive for persistence\n",
        "GOOGLE_DRIVE_ROOT = \"/content/drive/My Drive\"\n",
        "EMBEDDINGS_DIR_ON_DRIVE = os.path.join(GOOGLE_DRIVE_ROOT, \"ML_Embeddings_SBIR_PyTorch\")\n",
        "os.makedirs(EMBEDDINGS_DIR_ON_DRIVE, exist_ok=True) # Ensure this directory exists\n",
        "PROJECTION_NET_SAVE_PATH = os.path.join(EMBEDDINGS_DIR_ON_DRIVE, \"sketch_projection_net.pth\")\n",
        "GALLERY_EMBEDDINGS_FILE = os.path.join(EMBEDDINGS_DIR_ON_DRIVE, \"gallery_embeddings.npy\")\n",
        "GALLERY_IMAGE_PATHS_FILE = os.path.join(EMBEDDINGS_DIR_ON_DRIVE, \"gallery_image_filepaths.npy\")\n",
        "\n",
        "if os.path.exists(PROJECTION_NET_SAVE_PATH):\n",
        "    projection_net.load_state_dict(torch.load(PROJECTION_NET_SAVE_PATH, map_location=device))\n",
        "    projection_net.eval() # Set to evaluation mode (important for dropout/batchnorm layers if present)\n",
        "    print(f\"Sketch Projection Network loaded from: {PROJECTION_NET_SAVE_PATH}\")\n",
        "else:\n",
        "    print(f\"Error: Trained Sketch Projection Network not found at {PROJECTION_NET_SAVE_PATH}\")\n",
        "    print(\"Please run the training cell (Cell 4) first and ensure it completes successfully.\")\n",
        "    exit()\n",
        "\n",
        "# --- Load Your Image Gallery Embeddings (using original CLIP Image Encoder) ---\n",
        "# This part is the same as before, using the frozen CLIP model.\n",
        "gallery_embeddings = None\n",
        "gallery_image_filepaths = None\n",
        "\n",
        "\n",
        "if os.path.exists(GALLERY_EMBEDDINGS_FILE) and os.path.exists(GALLERY_IMAGE_PATHS_FILE):\n",
        "    print(f\"\\nLoading pre-saved gallery embeddings from {EMBEDDINGS_DIR_ON_DRIVE}...\")\n",
        "    gallery_embeddings = np.load(GALLERY_EMBEDDINGS_FILE)\n",
        "    gallery_image_filepaths = np.load(GALLERY_IMAGE_PATHS_FILE, allow_pickle=True)\n",
        "    print(f\"Loaded gallery embeddings shape: {gallery_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"\\nGallery embeddings not found. Attempting to build them now (this may take a while)...\")\n",
        "    gallery_embeddings_list = []\n",
        "    gallery_image_filepaths_list = []\n",
        "\n",
        "    IMAGE_GALLERY_PATH = os.path.join(BASE_DATA_DIR, \"256x256\", \"photo\", \"tx_000100000000\") # Ensure this is defined\n",
        "    if not os.path.exists(IMAGE_GALLERY_PATH):\n",
        "        print(f\"Error: Image gallery path does not exist for building: {IMAGE_GALLERY_PATH}\")\n",
        "        print(\"Please ensure your dataset is downloaded/unzipped correctly and paths are set correctly.\")\n",
        "        exit()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for category_name in tqdm(os.listdir(IMAGE_GALLERY_PATH), desc=\"Processing gallery images\"):\n",
        "            category_path = os.path.join(IMAGE_GALLERY_PATH, category_name)\n",
        "            if not os.path.isdir(category_path):\n",
        "                continue\n",
        "\n",
        "            for image_filename in os.listdir(category_path):\n",
        "                if image_filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_filepath = os.path.join(category_path, image_filename)\n",
        "                    try:\n",
        "                        image = Image.open(image_filepath).convert(\"RGB\")\n",
        "                        inputs = processor(images=image, return_tensors=\"pt\")\n",
        "                        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                        image_feature = model.get_image_features(**inputs).detach().cpu().numpy()\n",
        "                        gallery_embeddings_list.append(image_feature)\n",
        "                        gallery_image_filepaths_list.append(image_filepath)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Could not process image {image_filepath}: {e}\")\n",
        "\n",
        "    if gallery_embeddings_list:\n",
        "        gallery_embeddings = np.vstack(gallery_embeddings_list)\n",
        "\n",
        "        gallery_image_filepaths = np.array(gallery_image_filepaths_list)\n",
        "        print(f\"Encoded {len(gallery_embeddings_list)} gallery images. Embeddings shape: {gallery_embeddings.shape}\")\n",
        "\n",
        "        np.save(GALLERY_EMBEDDINGS_FILE, gallery_embeddings)\n",
        "        np.save(GALLERY_IMAGE_PATHS_FILE, gallery_image_filepaths)\n",
        "        print(f\"Gallery embeddings saved to {EMBEDDINGS_DIR_ON_DRIVE}\")\n",
        "    else:\n",
        "        print(\"No images processed in the gallery. Check your IMAGE_GALLERY_PATH and dataset contents.\")\n",
        "        exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJy8D6hwl6gd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# --- Device ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Paths (update these for your environment) ---\n",
        "TEST_SKETCH_PATH = os.path.join(BASE_DATA_DIR, \"256x256\", \"splitted_sketches\", \"test\", \"tx_000100000000\")\n",
        "TEST_PHOTO_PATH = os.path.join(BASE_DATA_DIR, \"256x256\", \"photo\", \"tx_000100000000\")\n",
        "SPLIT_SAVE_PATH = os.path.join(EMBEDDINGS_DIR_ON_DRIVE, \"category_split.json\")\n",
        "PROJECTION_NET_SAVE_PATH = os.path.join(EMBEDDINGS_DIR_ON_DRIVE, \"sketch_projection_net_category_infonce.pth\")\n",
        "\n",
        "# --- Load category split ---\n",
        "with open(SPLIT_SAVE_PATH, \"r\") as f:\n",
        "    split = json.load(f)\n",
        "test_categories = split[\"test_categories\"]\n",
        "\n",
        "# --- Load CLIP model ---\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "CLIP_EMBEDDING_DIM = 512\n",
        "\n",
        "# --- Define SketchProjectionNet ---\n",
        "class SketchProjectionNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SketchProjectionNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --- Load Projection Network ---\n",
        "projection_net = SketchProjectionNet(CLIP_EMBEDDING_DIM, CLIP_EMBEDDING_DIM).to(device)\n",
        "projection_net.load_state_dict(torch.load(PROJECTION_NET_SAVE_PATH, map_location=device))\n",
        "projection_net.eval()\n",
        "\n",
        "# --- Build gallery embeddings ---\n",
        "print(\"Precomputing gallery embeddings for test categories...\")\n",
        "\n",
        "gallery_image_filepaths = []\n",
        "gallery_embeddings = []\n",
        "\n",
        "for category in tqdm(test_categories, desc=\"Encoding gallery\"):\n",
        "    cat_path = os.path.join(TEST_PHOTO_PATH, category)\n",
        "    if not os.path.isdir(cat_path):\n",
        "        continue\n",
        "\n",
        "    image_files = glob.glob(os.path.join(cat_path, '*'))\n",
        "    for img_file in image_files:\n",
        "        try:\n",
        "            img = Image.open(img_file).convert(\"RGB\")\n",
        "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embed = model.get_image_features(**inputs)\n",
        "                embed = F.normalize(embed, p=2, dim=-1).cpu().numpy()\n",
        "\n",
        "            gallery_embeddings.append(embed)\n",
        "            gallery_image_filepaths.append(img_file)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing gallery image {img_file}: {e}\")\n",
        "\n",
        "gallery_embeddings = np.concatenate(gallery_embeddings, axis=0)\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"\\nRunning full benchmark on test split...\")\n",
        "\n",
        "correct_top1 = 0\n",
        "all_aps = []\n",
        "total_sketches = 0\n",
        "\n",
        "for category in tqdm(test_categories, desc=\"Evaluating sketches\"):\n",
        "    cat_path = os.path.join(TEST_SKETCH_PATH, category)\n",
        "    if not os.path.isdir(cat_path):\n",
        "        continue\n",
        "\n",
        "    sketch_files = glob.glob(os.path.join(cat_path, \"*.png\")) + \\\n",
        "                   glob.glob(os.path.join(cat_path, \"*.jpg\")) + \\\n",
        "                   glob.glob(os.path.join(cat_path, \"*.jpeg\"))\n",
        "\n",
        "    for sketch_file in sketch_files:\n",
        "        try:\n",
        "            sketch_image = Image.open(sketch_file).convert(\"RGB\")\n",
        "            inputs = processor(images=sketch_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                sketch_embed = model.get_image_features(**inputs)\n",
        "                projected_embed = projection_net(sketch_embed)\n",
        "                projected_embed = F.normalize(projected_embed, p=2, dim=-1).cpu().numpy()\n",
        "\n",
        "            # Cosine similarity\n",
        "            sims = cosine_similarity(projected_embed, gallery_embeddings)[0]\n",
        "            sorted_indices = np.argsort(sims)[::-1]\n",
        "\n",
        "            # Top-1 prediction\n",
        "            top1_image_path = gallery_image_filepaths[sorted_indices[0]]\n",
        "            top1_category = os.path.basename(os.path.dirname(top1_image_path))\n",
        "\n",
        "            if top1_category == category:\n",
        "                correct_top1 += 1\n",
        "\n",
        "            # mAP calculation\n",
        "            true_labels = np.array([\n",
        "                1 if os.path.basename(os.path.dirname(img_path)) == category else 0\n",
        "                for img_path in gallery_image_filepaths\n",
        "            ])\n",
        "            ap = average_precision_score(true_labels, sims)\n",
        "            all_aps.append(ap)\n",
        "\n",
        "            total_sketches += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sketch {sketch_file}: {e}\")\n",
        "\n",
        "# --- Final Metrics ---\n",
        "rank1_acc = correct_top1 / total_sketches * 100 if total_sketches > 0 else 0\n",
        "mean_ap = np.mean(all_aps) * 100 if all_aps else 0\n",
        "\n",
        "print(\"\\nBenchmark Results:\")\n",
        "print(f\"Total Test Sketches Evaluated: {total_sketches}\")\n",
        "print(f\"Rank-1 Accuracy: {rank1_acc:.2f}%\")\n",
        "print(f\"Mean Average Precision (mAP): {mean_ap:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zKyuRGTz2FmU"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWgDcRcYydR4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import average_precision_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import faiss\n",
        "\n",
        "GALLERY_EMBEDDINGS_SAVE_PATH = \"/content/drive/My Drive/gallery_embeddings.npz\"\n",
        "\n",
        "# --- Device Configuration ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Define SketchProjectionNet ---\n",
        "class SketchProjectionNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SketchProjectionNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --- Load Pre-trained CLIP Model (Frozen) ---\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\",)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "CLIP_EMBEDDING_DIM = 512\n",
        "\n",
        "# --- Paths ---\n",
        "TU_BERLIN_SKETCH_PATH = os.path.join(\"/content/data/TUBerlin\", \"png_ready\")\n",
        "TU_BERLIN_IMAGE_PATH = os.path.join(\"/content/data/TUBerlin\", \"ImageResized_ready\")\n",
        "PROJECTION_NET_PATH = \"/content/drive/MyDrive/ML_Embeddings_SBIR_PyTorch/sketch_projection_net_category_infonce.pth\"\n",
        "\n",
        "# --- Load TU-Berlin categories ---\n",
        "tu_berlin_categories = sorted(os.listdir(TU_BERLIN_SKETCH_PATH))\n",
        "\n",
        "# --- Load Projection Network ---\n",
        "projection_net = SketchProjectionNet(CLIP_EMBEDDING_DIM, CLIP_EMBEDDING_DIM).to(device)\n",
        "projection_net.load_state_dict(torch.load(PROJECTION_NET_PATH, map_location=device))\n",
        "projection_net.eval()\n",
        "\n",
        "# --- Build or Load gallery embeddings ---\n",
        "if os.path.exists(GALLERY_EMBEDDINGS_SAVE_PATH):\n",
        "    print(\"Loading precomputed gallery embeddings...\")\n",
        "    saved = np.load(GALLERY_EMBEDDINGS_SAVE_PATH, allow_pickle=True)\n",
        "    gallery_embeddings = saved['embeddings']\n",
        "    gallery_image_filepaths = saved['filepaths'].tolist()\n",
        "\n",
        "    # Ensure shape is (N,512)\n",
        "    if len(gallery_embeddings.shape) == 1:\n",
        "        gallery_embeddings = np.vstack(gallery_embeddings)\n",
        "\n",
        "    print(\"Loaded gallery embeddings with shape:\", gallery_embeddings.shape)\n",
        "\n",
        "else:\n",
        "    print(\"Precomputing gallery embeddings for TU-Berlin...\")\n",
        "\n",
        "    gallery_image_filepaths = []\n",
        "    gallery_embeddings = []\n",
        "\n",
        "    for category in tqdm(tu_berlin_categories, desc=\"Encoding gallery\"):\n",
        "        cat_path = os.path.join(TU_BERLIN_IMAGE_PATH, category)\n",
        "        if not os.path.isdir(cat_path):\n",
        "            continue\n",
        "\n",
        "        image_files = glob.glob(os.path.join(cat_path, '*'))\n",
        "        for img_file in image_files:\n",
        "            try:\n",
        "                img = Image.open(img_file).convert(\"RGB\")\n",
        "                inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    embed = model.get_image_features(**inputs)\n",
        "                    embed = F.normalize(embed, p=2, dim=-1).cpu().numpy()\n",
        "\n",
        "                gallery_embeddings.append(embed)\n",
        "                gallery_image_filepaths.append(img_file)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing gallery image {img_file}: {e}\")\n",
        "\n",
        "    gallery_embeddings = np.concatenate(gallery_embeddings, axis=0)\n",
        "    print(\"Final gallery embeddings shape:\", gallery_embeddings.shape)\n",
        "\n",
        "    np.savez(GALLERY_EMBEDDINGS_SAVE_PATH, embeddings=gallery_embeddings, filepaths=np.array(gallery_image_filepaths))\n",
        "    print(f\"Saved gallery embeddings to {GALLERY_EMBEDDINGS_SAVE_PATH}\")\n",
        "\n",
        "# --- Build FAISS Index for Exact Search ---\n",
        "index = faiss.IndexFlatIP(CLIP_EMBEDDING_DIM)\n",
        "index.add(gallery_embeddings.astype(np.float32))\n",
        "\n",
        "# --- Evaluation with batching ---\n",
        "print(\"\\nRunning zero-shot benchmark on TU-Berlin with batching and FAISS...\")\n",
        "\n",
        "correct_top1 = 0\n",
        "all_aps = []\n",
        "total_sketches = 0\n",
        "BATCH_SIZE = 32  # Reduced for stability\n",
        "MAX_K = min(1000, len(gallery_embeddings))  # Limit k to avoid RAM explosion\n",
        "\n",
        "for category in tqdm(tu_berlin_categories, desc=\"Evaluating sketches\"):\n",
        "    cat_path = os.path.join(TU_BERLIN_SKETCH_PATH, category)\n",
        "    if not os.path.isdir(cat_path):\n",
        "        continue\n",
        "\n",
        "    sketch_files = glob.glob(os.path.join(cat_path, \"*.png\")) + \\\n",
        "                   glob.glob(os.path.join(cat_path, \"*.jpg\")) + \\\n",
        "                   glob.glob(os.path.join(cat_path, \"*.jpeg\"))\n",
        "\n",
        "    # Process in batches\n",
        "    for i in range(0, len(sketch_files), BATCH_SIZE):\n",
        "        batch_files = sketch_files[i:i+BATCH_SIZE]\n",
        "        images = [Image.open(f).convert(\"RGB\") for f in batch_files]\n",
        "\n",
        "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sketch_embeds = model.get_image_features(**inputs)\n",
        "            projected_embeds = projection_net(sketch_embeds)\n",
        "            projected_embeds = F.normalize(projected_embeds, p=2, dim=-1).cpu().numpy()\n",
        "\n",
        "        # FAISS search with limited k\n",
        "        sims, indices = index.search(projected_embeds.astype(np.float32), k=MAX_K)\n",
        "\n",
        "        for j, sims_j in enumerate(sims):\n",
        "            sorted_indices = indices[j]\n",
        "            top1_image_path = gallery_image_filepaths[sorted_indices[0]]\n",
        "            top1_category = os.path.basename(os.path.dirname(top1_image_path))\n",
        "\n",
        "            if top1_category == category:\n",
        "                correct_top1 += 1\n",
        "\n",
        "            # mAP calculation (approximate over top-k for efficiency)\n",
        "            true_labels = np.array([\n",
        "              1 if os.path.basename(os.path.dirname(gallery_image_filepaths[idx])) == category else 0\n",
        "              for idx in sorted_indices\n",
        "            ])\n",
        "            if np.sum(true_labels) == 0:\n",
        "              # No positive sample retrieved; skip AP calculation for this sketch\n",
        "              continue\n",
        "\n",
        "            ap = average_precision_score(true_labels, sims_j)\n",
        "            all_aps.append(ap)\n",
        "\n",
        "            total_sketches += 1\n",
        "\n",
        "# --- Final Metrics ---\n",
        "rank1_acc = correct_top1 / total_sketches * 100 if total_sketches > 0 else 0\n",
        "mean_ap = np.mean(all_aps) * 100 if all_aps else 0\n",
        "\n",
        "print(\"\\nTU-Berlin Zero-Shot Benchmark Results (Batched + FAISS):\")\n",
        "print(f\"Total Test Sketches Evaluated: {total_sketches}\")\n",
        "print(f\"Rank-1 Accuracy: {rank1_acc:.2f}%\")\n",
        "print(f\"Mean Average Precision (mAP): {mean_ap:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
